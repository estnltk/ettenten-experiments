{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# source: https://github.com/estnltk/estnltk/blob/devel_1.6/estnltk/converters/cg3_annotation_parser.py\n",
    "# code copied here temporally\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import Tuple, Optional\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "def get_reversed_mapping(cats) -> dict:\n",
    "    \"\"\"\n",
    "    Maps cats as 'form_value': 'category'.\n",
    "    \"\"\"\n",
    "    reversed_cats_mapping = defaultdict(lambda: 'unknown_attribute')\n",
    "    for cat, forms in cats.items():\n",
    "        for form in forms:\n",
    "            if form in reversed_cats_mapping:\n",
    "                raise Exception(\"(!)Can't map form: '%s' and category: '%s'\" % (form, cat))\n",
    "            reversed_cats_mapping[form] = cat\n",
    "    return reversed_cats_mapping\n",
    "\n",
    "\n",
    "def get_cats(cats):\n",
    "    \"\"\"\n",
    "    Checks that attribute is valid.\n",
    "    \"\"\"\n",
    "    wrong_cats = ['lemma', 'ending', 'partofspeech', 'deprel', 'head']\n",
    "    for key in cats.keys():\n",
    "        if key in wrong_cats:\n",
    "            raise Exception('(!)Wrong attribute for feature category: \"%s\"' % (key))\n",
    "    return cats\n",
    "\n",
    "\n",
    "class CG3AnnotationParser:\n",
    "    \"\"\"\n",
    "    Parser for parsing visl cg3 format analysis line.\n",
    "\n",
    "    Example:\n",
    "        line = '\t\"mari\" Lle S com sg all cap @ADVL #1->4'\n",
    "        parser = CG3AnnotationParser()\n",
    "        processed_line = parser.parse(line)\n",
    "\n",
    "    Output:\n",
    "        {'lemma': 'mari', 'head': '#1->4', 'partofspeech': 'S', 'feats': \\\n",
    "        OrderedDict([('substantive_type', ['com']), ('number', ['sg']), ('case', ['all']), ('capitalized', ['cap'])]), \\\n",
    "        # 'deprel': '@ADVL ', 'ending': 'le'}\n",
    "    \"\"\"\n",
    "    pat_analysis_line = re.compile('^\\s+\"(?P<lemma>.+)\" (?P<ending>L\\w+)*(?P<cats>[^@#]*)(?P<syntax>[@#]*.*)$')\n",
    "    pat_pos_form = re.compile('^ *[A-Z]*\\s*(?P<form>[^#@]*).*$')\n",
    "    pat_form_pos = re.compile('^ *(?P<form>[a-z]+) (?P<postag>[A-Z]).*$')\n",
    "\n",
    "    cats = {'case': {'nom', 'gen', 'part', 'ill', 'in', 'el', 'all', 'ad', 'abl',\n",
    "                     'tr', 'term', 'es', 'abes', 'kom', 'adit'},\n",
    "            'number': {'sg', 'pl'},\n",
    "            'voice': {'imps', 'ps'},\n",
    "            'tense': {'pres', 'past', 'impf'},\n",
    "            'mood': {'indic', 'cond', 'imper', 'quot'},\n",
    "            'person': {'ps1', 'ps2', 'ps3'},\n",
    "            'polarity': {'af', 'neg'},\n",
    "            'inf_form': {'sup', 'inf', 'ger', 'partic'},\n",
    "            'subtype': {'pos', 'det', 'refl', 'dem', 'inter_rel', 'pers', 'rel', 'rec', 'indef', 'comp', 'super',\n",
    "                        'main', 'mod', 'aux', 'prop', 'com', 'card', 'ord', 'pre', 'post', 'crd', 'sub', 'adjectival',\n",
    "                        'adverbial', 'nominal', 'verbal', 'Col', 'Com', 'Cpr', 'Cqu', 'Csq', 'Dsd', 'Dsh', 'Ell',\n",
    "                        'Els', 'Exc', 'Fst', 'Int', 'Opr', 'Oqu', 'Osq', 'Quo', 'Scl', 'Sla','Sml'},\n",
    "            'number_format': {'l', 'roman', 'digit'},\n",
    "            'capitalized': {'cap'},\n",
    "            'finiteness': {'<FinV>', '<Inf>', '<InfP>'},\n",
    "            'subcat': {'<Abl>', '<Ad>', '<All>', '<El>', '<Es>', '<Ill>', '<In>', '<Kom>', '<Part>',\n",
    "                       '<all>', '<el>', '<gen>', '<ja>', '<kom>', '<mata>', '<mine>', '<nom>', '<nu>',\n",
    "                       '<nud>', '<part>', '<tav>', '<tu>', '<tud>', '<v>', '<Ter>', '<Tr>', '<Intr>',\n",
    "                       '<NGP-P>', '<NGP>', '<Part-P>'},\n",
    "            'clause_boundary': {'CLB'}}\n",
    "\n",
    "    reversed_cats = get_cats(get_reversed_mapping(cats))\n",
    "\n",
    "    def __init__(self, supress_exceptions=False):\n",
    "        self.supress_exceptions = supress_exceptions\n",
    "\n",
    "    @staticmethod\n",
    "    def get_forms(cats, postag, line):\n",
    "        \"\"\"\n",
    "        Creates morphological features from morphological categories.\n",
    "        \"\"\"\n",
    "        assert isinstance(cats, str), '(!)Unexpected type for \"cats\" argument! Expected a string.'\n",
    "        assert isinstance(postag, str), '(!)Unexpected type for \"postag\" argument! Expected a string.'\n",
    "        assert isinstance(line, str), '(!)Unexpected type for \"line\" argument! Expected a string.'\n",
    "        forms = []\n",
    "        # Features matching\n",
    "        m1 = CG3AnnotationParser.pat_pos_form.match(cats)\n",
    "        m2 = CG3AnnotationParser.pat_form_pos.match(cats)\n",
    "        if m1:\n",
    "            forms = (m1.group('form')).split()\n",
    "        elif m2:\n",
    "            forms = (m2.group('form')).split()\n",
    "            postag = m2.group('postag')\n",
    "        else:\n",
    "            print('(!) Unexpected format of analysis line: ' + line, file=sys.stderr)\n",
    "        return forms, postag\n",
    "\n",
    "    @staticmethod\n",
    "    def get_analysed_forms(forms):\n",
    "        \"\"\"\n",
    "        Finds attribute to each feature element.\n",
    "        \"\"\"\n",
    "        assert isinstance(forms, list), '(!)Unexpected type for \"forms\" argument! Expected a list.'\n",
    "        analysed_forms = {}\n",
    "        for form in forms:\n",
    "            if CG3AnnotationParser.reversed_cats[form] in analysed_forms:\n",
    "                analysed_forms[CG3AnnotationParser.reversed_cats[form]].append(form)\n",
    "            else:\n",
    "                analysed_forms[CG3AnnotationParser.reversed_cats[form]] = [form]\n",
    "        return analysed_forms\n",
    "\n",
    "    @staticmethod\n",
    "    def get_postag(cats):\n",
    "        \"\"\"\n",
    "        Finds postag from categories value.\n",
    "        \"\"\"\n",
    "        assert isinstance(cats, str), '(!)Unexpected type for \"cats\" argument! Expected a string.'\n",
    "        if cats.startswith('Z '):\n",
    "            postag = 'Z'\n",
    "        elif cats.endswith('D '):\n",
    "            postag = 'D'\n",
    "        else:\n",
    "            postag = (cats.split())[0] if len(cats.split()) >= 1 and len(cats.split()[0]) == 1  else 'X'\n",
    "        return postag\n",
    "\n",
    "    @staticmethod\n",
    "    def get_syntax(syntax_analysis):\n",
    "        \"\"\"\n",
    "        Finds deprel and head value if analysis has this info.\n",
    "        \"\"\"\n",
    "        assert isinstance(syntax_analysis, str), '(!)Unexpected type for \"syntax_analysis\" argument! Expected a string.'\n",
    "        syntax_chunks = syntax_analysis.split('#')\n",
    "        deprel = syntax_chunks[0].strip(' ').split()\n",
    "        id, head = syntax_chunks[1].split('->')\n",
    "        if deprel == []:\n",
    "            deprel = '_'\n",
    "        return deprel, id, head\n",
    "\n",
    "    def split_visl_analysis_line(self, line: str) -> Optional[Tuple[str]]:\n",
    "        \"\"\"\n",
    "        Splits visl row analysis into four pieces 'lemma', 'ending', 'categories' and 'syntactical info'.\n",
    "        If some info is missing, it's returned as ''.\n",
    "        \"\"\"\n",
    "        assert isinstance(line, str), '(!)Unexpected type for \"line\" argument! Expected a string.'\n",
    "        analysis_match = CG3AnnotationParser.pat_analysis_line.match(line)\n",
    "        if not analysis_match:\n",
    "            if line.startswith('  ') or line.startswith('\\t'):\n",
    "                if self.supress_exceptions:\n",
    "                    return tuple(['', '', '', ''])\n",
    "                else:\n",
    "                    raise Exception('(!) Malformed analysis line: ' + line)\n",
    "            return tuple(['', '', '', ''])\n",
    "        return tuple(analysis_match.groups(default=''))\n",
    "\n",
    "    def parse(self, line) -> dict:\n",
    "        \"\"\"\n",
    "        Processes visl analysis line.\n",
    "        \"\"\"\n",
    "        assert isinstance(line, str), '(!) Unexpected type of input argument! Expected a string.'\n",
    "        if not (line.startswith('  ') or line.startswith('\\t')):\n",
    "            if self.supress_exceptions:\n",
    "                return {}\n",
    "            else:\n",
    "                raise Exception('(!) Unexpected analysis line: ' + line)\n",
    "        analysis = self.split_visl_analysis_line(line)\n",
    "        lemma, ending, cats, syntax = analysis\n",
    "        ending = ending.lstrip('L') if ending else '_'\n",
    "        postag = self.get_postag(cats)\n",
    "        forms, postag = self.get_forms(cats, postag, line)\n",
    "        analysed_forms = self.get_analysed_forms(forms)\n",
    "        # Visl row with syntactic analysis, e.g. \"ole\" Ln V main indic pres ps1 sg ps af @FMV #3->0\n",
    "        if '#' in analysis[3]:\n",
    "            deprel, id, head = self.get_syntax(syntax)\n",
    "            return {'id': id, 'lemma': lemma, 'ending': ending, 'partofspeech': postag,\n",
    "                    'deprel': deprel,\n",
    "                    'head': head, **analysed_forms}\n",
    "        # Visl row with verb or adpositions info, e.g. \"ole\" Ln V main indic pres ps1 sg ps af <FinV> <Intr>\n",
    "        else:\n",
    "            return {'lemma': lemma, 'ending': ending, 'partofspeech': postag, 'deprel': '_',\n",
    "                    'head': '_', **analysed_forms}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from estnltk import Text\n",
    "from estnltk.taggers import VabamorfTagger\n",
    "import re\n",
    "from estnltk.vabamorf.morf import synthesize\n",
    "from Levenshtein import distance\n",
    "import sys\n",
    "\n",
    "morph_tagger = VabamorfTagger(guess=False,propername=False,disambiguate=False,phonetic=False)\n",
    "\n",
    "cwd = os.getcwd()\n",
    "path = os.path.join(cwd, \"ettenten-valik\") \n",
    "new_path = os.path.join(\"ettenten-valik_tsv\",'') \n",
    "\n",
    "cases={\"abes\":\"ab\",\"adit\":\"adt\",\"gen\":\"g\",\"nom\":\"n\",\"part\":\"p\",\"term\":\"ter\"}\n",
    "\n",
    "manual_corrections={\"süüa-juua\":\"süüa-juua\"+\"\\t\"+\"süüa-juua\"+\"\\t\"+\"Ok\",\"L-S\":\"L-S\"+\"\\t\"+\"L-S\"+\"\\t\"+\"Name\",\n",
    "              \"New Yorgist\":\"New Yorgist\"+\"\\t\"+\"New Yorgist\"+\"\\t\"+\"Name\",\"20D/30D\":\"20D/30D\"+\"\\t\"+\"20D/30D\"+\"\\t\"+\"Abbrev_Num\",\n",
    "             \"mp3-dega\":\"mp3-dega\"+\"\\t\"+\"mp3-dega\"+\"\\t\"+\"Abbrev_Num\",\n",
    "             \"mp3-de\":\"mp3-de\"+\"\\t\"+\"mp3-de\"+\"\\t\"+\"Abbrev_Num\",\n",
    "             \"S3-el\":\"S3-el\"+\"\\t\"+\"S3-l\"+\"\\t\"+\"Abbrev_Num\",\"nr,`d\":\"nr,`d\"+\"\\t\"+\"nr-d\"+\"\\t\"+\"Abbrev\",\n",
    "              \"}-ks\":\"}-ks\"+\"\\t\"+\"}-ks\"+\"\\t\"+\"Abbrev\",\"jämedad/paksud\":\"jämedad/paksud\"+\"\\t\"+\"jämedad/paksud\"+\"\\t\"+\"Ok\",\n",
    "             \"s.-tapead\":\"s.-tapead\"+\"\\t\"+\"sitapead\"+\"\\t\"+\"Spell_ED>1\",\n",
    "             \"suhet-peret\":\"suhet-peret\"+\"\\t\"+\"suhet-peret\"+\"\\t\"+\"Ok\",\n",
    "             \"sidemeid-tutvusi\":\"sidemeid-tutvusi\"+\"\\t\"+\"sidemeid-tutvusi\"+\"\\t\"+\"Ok\",\n",
    "             \"6-megane\":\"6-megane\"+\"\\t\"+\"6-megane\"+\"\\t\"+\"Ok\",\n",
    "             \"e-teenindusse\":\"e-teenindusse\"+\"\\t\"+\"e-teenindusse\"+\"\\t\"+\"Ok\",\n",
    "             \"a-seadmetega\":\"a-seadmetega\"+\"\\t\"+\"a-seadmetega\"+\"\\t\"+\"Ok\",\"15-ndal\":\"15-ndal\"+\"\\t\"+\"15-ndal\"+\"\\t\"+\"Ok\",\n",
    "              \"5-ndale\":\"5-ndale\"+\"\\t\"+\"5-ndale\"+\"\\t\"+\"Ok\",\"Võib-olla\":\"Võib-olla\"+\"\\t\"+\"Võib-olla\"+\"\\t\"+\"Ok\",\n",
    "              \"30-aastaselt\":\"30-aastaselt\"+\"\\t\"+\"30-aastaselt\"+\"\\t\"+\"Ok\",\n",
    "              \"-ah\":\"-ah\"+\"\\t\"+\"ah\"+\"\\t\"+\"Spell_ED_1\",\"ja/või\":\"ja/või\"+\"\\t\"+\"ja/või\"+\"\\t\"+\"Ok\",\n",
    "              \"kirelt\":\"kirelt\"+\"\\t\"+\"kiirelt\"+\"\\t\"+\"Spell_ED_1\",\"Vaepeal\":\"Vaepeal\"+\"\\t\"+\"Vahepeal\"+\"\\t\"+\"Spell_ED_1\",\n",
    "              \"Aitähh\":\"Aitähh\"+\"\\t\"+\"Aitäh\"+\"\\t\"+\"Spell_ED_1\",\"aitähh\":\"aitähh\"+\"\\t\"+\"aitäh\"+\"\\t\"+\"Spell_ED_1\",\n",
    "              \"ara\":\"ara\"+\"\\t\"+\"ära\"+\"\\t\"+\"Spell_Missing_Diacritics\",\"miskiparast\":\"miskiparast\"+\"\\t\"+\"miskipärast\"+\"\\t\"+\"Spell_Missing_Diacritics\",\n",
    "              \"A\":\"A\"+\"\\t\"+\"A\"+\"\\t\"+\"Ok\",\"mai\":\"mai\"+\"\\t\"+\"ma ei\"+\"\\t\"+\"Spell_ED>1\",\"Mai\":\"Mai\"+\"\\t\"+\"ma ei\"+\"\\t\"+\"Spell_ED>1\",\n",
    "              \"väljavalitud\":\"väljavalitud\"+\"\\t\"+\"välja valitud\"+\"\\t\"+\"Spell_Missing_Space\",\n",
    "              \"Me\":\"Me\"+\"\\t\"+\"Me\"+\"\\t\"+\"Name\",\n",
    "              \"L-S\":\"L-S\"+\"\\t\"+\"L-S\"+\"\\t\"+\"Name\",\"pealgi\":\"pealgi\"+\"\\t\"+\"pealegi\"+\"\\t\"+\"Spell_ED_1\",\n",
    "              \"kiirest\":\"kiirest\"+\"\\t\"+\"kiiresti\"+\"\\t\"+\"Spell_ED_1\",\n",
    "              \"alti\":\"alti\"+\"\\t\"+\"alati\"+\"\\t\"+\"Spell_ED_1\",\"mittekeegi\":\"mittekeegi\"+\"\\t\"+\"mitte keegi\"+\"\\t\"+\"Spell_Missing_Space\",\n",
    "              \"õppinud\":\"õppinud\"+\"\\t\"+\"õppinud\"+\"\\t\"+\"Ok\",\"põhjendatud\":\"põhjendatud\"+\"\\t\"+\"põhjendatud\"+\"\\t\"+\"Ok\",\n",
    "              \"syya\":\"syya\"+\"\\t\"+\"süüa\"+\"\\t\"+\"Spell_Missing_Diacritics\",\"oller\":\"oller\"+\"\\t\"+\"olles\"+\"\\t\"+\"Spell_ED_1\",\n",
    "              \"väljaminnes\":\"väljaminnes\"+\"\\t\"+\"välja minnes\"+\"\\t\"+\"Spell_Missing_Space\",\n",
    "              \"poleksi\":\"poleksi\"+\"\\t\"+\"polekski\"+\"\\t\"+\"Spell_ED_1\",\n",
    "              \"tia\":\"tia\"+\"\\t\"+\"tea\"+\"\\t\"+\"Spell_ED_1\",\n",
    "              \"kmaksis\":\"kmaksis\"+\"\\t\"+\"maksis\"+\"\\t\"+\"Spell_ED_1\",\"käisgi\":\"käisgi\"+\"\\t\"+\"käiski\"+\"\\t\"+\"Spell_ED_1\",\n",
    "              \"vötta\":\"vötta\"+\"\\t\"+\"võtta\"+\"\\t\"+\"Spell_Changed_Diacritics\",\"ple\":\"ple\"+\"\\t\"+\"pole\"+\"\\t\"+\"Spell_ED_1\",\n",
    "              \"plee\":\"plee\"+\"\\t\"+\"pole\"+\"\\t\"+\"Spell_ED>1\",\"SuperCruise\":\"SuperCruise\"+\"\\t\"+\"SuperCruise\"+\"\\t\"+\"Name\",\n",
    "              \"Tamakasuga\":\"Tamakasuga\"+\"\\t\"+\"Tamakasuga\"+\"\\t\"+\"Name\",\"mp3-d\":\"mp3-d\"+\"\\t\"+\"mp3-d\"+\"\\t\"+\"Abbrev_Num\",\n",
    "              \"4s-il\":\"4s-il\"+\"\\t\"+\"4s-il\"+\"\\t\"+\"Abbrev_Num\",\"mp3-l\":\"mp3-l\"+\"\\t\"+\"mp3-l\"+\"\\t\"+\"Abbrev_Num\",\n",
    "              \"rummstain_DuHast_(LIVE).mp3-ga\":\"rummstain_DuHast_(LIVE).mp3-ga\"+\"\\t\"+\"rummstain_DuHast_(LIVE).mp3-ga\"+\"\\t\"+\"Abbrev_Num\",\n",
    "              \"LV`se\":\"LV`se\"+\"\\t\"+\"LV\"+\"\\t\"+\"Abbrev\",\"LV`sel\":\"LV`sel\"+\"\\t\"+\"LV-l\"+\"\\t\"+\"Abbrev\",\n",
    "              \"msnnis\":\"msnnis\"+\"\\t\"+\"msnis\"+\"\\t\"+\"Abbrev\",\"msnis\":\"msnis\"+\"\\t\"+\"msnis\"+\"\\t\"+\"Abbrev\",\"CMOS-ga\":\"CMOS-ga\"+\"\\t\"+\"CMOS-ga\"+\"\\t\"+\"Abbrev\",\n",
    "              \"SEO’st\":\"SEO’st\"+\"\\t\"+\"SEO’st\"+\"\\t\"+\"Abbrev\",\"M-st\":\"M-st\"+\"\\t\"+\"M-st\"+\"\\t\"+\"Abbrev\",       \n",
    "             }\n",
    "\n",
    "\n",
    "parser = CG3AnnotationParser()\n",
    "\n",
    "def use_edit_distance(analysis, form, i, i2, mult_anal):\n",
    "    if analysis[\"case\"][0] in cases.keys():\n",
    "        form=analysis[\"number\"][0]+ \" \" + cases[analysis[\"case\"][0]]\n",
    "        new_word=synthesize(analysis[\"lemma\"], form=form)\n",
    "    else:\n",
    "        form=analysis[\"number\"][0] + \" \" + analysis[\"case\"][0]\n",
    "        new_word=synthesize(analysis[\"lemma\"], form=form)\n",
    "    if len(new_word)>0:\n",
    "        if i.lower() in new_word:\n",
    "            info=i+\"\\t\"+i+\"\\t\"+\"Ok\"\n",
    "            lines_list.append(info)\n",
    "        elif analysis[\"case\"][0] ==\"gen\" or analysis[\"case\"][0] ==\"part\":\n",
    "            if new_word[-1][-1] in [\"a\",\"e\",\"i\",\"o\",\"u\",\"õ\",\"ä\",\"ö\"] and new_word[-1][-1]!=i[-1]:\n",
    "                if analysis[\"case\"][0]==\"gen\":\n",
    "                    info=i+\"\\t\"+new_word[-1]+\"\\t\"+\"Spell_Unknown_Gen\"\n",
    "                    lines_list.append(info)\n",
    "                else:\n",
    "                    info=i+\"\\t\"+new_word[-1]+\"\\t\"+\"Spell_Unknown_Part\"\n",
    "                    lines_list.append(info)\n",
    "            elif len(re.findall(\"ä|Ä|ö|Ö|ü|Ü|Õ|õ\", i)) < len(re.findall(\"ä|Ä|ö|Ö|ü|Ü|Õ|õ\", new_word[-1])):\n",
    "                info=i+\"\\t\"+new_word[-1]+\"\\t\"+\"Spell_Missing_Diacritics\"\n",
    "                lines_list.append(info)\n",
    "            else:\n",
    "                ld=distance(i.lower(), new_word[-1].lower())\n",
    "                if ld > 1:\n",
    "                    new_form=analysis[\"ending\"].replace(\"_\",\"\")\n",
    "                    new_word2 = analysis[\"lemma\"]+new_form\n",
    "                    if new_word2.lower() == i.lower():\n",
    "                        if i[0].isupper():\n",
    "                            info=i+\"\\t\"+new_word2.capitalize()+\"\\t\"+\"Ok\"\n",
    "                            lines_list.append(info)\n",
    "                        else:\n",
    "                            info=i+\"\\t\"+new_word2+\"\\t\"+\"Ok\"\n",
    "                            lines_list.append(info)\n",
    "                    else:\n",
    "                        if i[0].isupper():\n",
    "                            info=i+\"\\t\"+new_word[-1].capitalize()+\"\\t\"+\"Spell_ED>1\"\n",
    "                            lines_list.append(info)\n",
    "                        else:\n",
    "                            info=i+\"\\t\"+new_word[-1]+\"\\t\"+\"Spell_ED>1\"\n",
    "                            lines_list.append(info)\n",
    "                elif ld == 1:\n",
    "                    if i[0].isupper():\n",
    "                        info=i+\"\\t\"+new_word[-1].capitalize()+\"\\t\"+\"Spell_ED_1\"\n",
    "                        lines_list.append(info)\n",
    "                    else:\n",
    "                        info=i+\"\\t\"+new_word[-1]+\"\\t\"+\"Spell_ED_1\"\n",
    "                        lines_list.append(info)\n",
    "                elif ld == 0:\n",
    "                    if i[0].isupper():\n",
    "                        info=i+\"\\t\"+new_word[-1].capitalize()+\"\\t\"+\"Ok\"\n",
    "                        lines_list.append(info)\n",
    "                    else:\n",
    "                        info=i+\"\\t\"+new_word[-1]+\"\\t\"+\"Ok\"\n",
    "                        lines_list.append(info)                          \n",
    "                            \n",
    "        else:\n",
    "            ld=distance(i.lower(), new_word[-1].lower())\n",
    "            if len(re.findall(\"ä|Ä|ö|Ö|ü|Ü|Õ|õ\", i)) < len(re.findall(\"ä|Ä|ö|Ö|ü|Ü|Õ|õ\", new_word[-1])):\n",
    "                info=i+\"\\t\"+new_word[-1]+\"\\t\"+\"Spell_Missing_Diacritics\"\n",
    "                lines_list.append(info)  \n",
    "            elif ld > 1:\n",
    "                new_form=analysis[\"ending\"].replace(\"_\",\"\")\n",
    "                new_word2 = analysis[\"lemma\"]+new_form\n",
    "                if new_word2.lower() == i.lower():\n",
    "                    if i[0].isupper():\n",
    "                        info=i+\"\\t\"+new_word2.capitalize()+\"\\t\"+\"Ok\"\n",
    "                        lines_list.append(info)\n",
    "                    else:\n",
    "                        info=i+\"\\t\"+new_word2+\"\\t\"+\"Ok\"\n",
    "                        lines_list.append(info)\n",
    "                else:\n",
    "                    if i[0].isupper():\n",
    "                        info=i+\"\\t\"+new_word[-1].capitalize()+\"\\t\"+\"Spell_ED>1\"\n",
    "                        lines_list.append(info)\n",
    "                    else:\n",
    "                        info=i+\"\\t\"+new_word[-1]+\"\\t\"+\"Spell_ED>1\"\n",
    "                        lines_list.append(info)\n",
    "            elif ld == 1:\n",
    "                if i[0].isupper():\n",
    "                    info=i+\"\\t\"+new_word[-1].capitalize()+\"\\t\"+\"Spell_ED_1\"\n",
    "                    lines_list.append(info)\n",
    "                else:\n",
    "                    info=i+\"\\t\"+new_word[-1]+\"\\t\"+\"Spell_ED_1\"\n",
    "                    lines_list.append(info)\n",
    "            elif ld == 0:\n",
    "                if i[0].isupper():\n",
    "                    info=i+\"\\t\"+new_word[-1].capitalize()+\"\\t\"+\"Ok\"\n",
    "                    lines_list.append(info)\n",
    "                else:\n",
    "                    info=i+\"\\t\"+new_word[-1]+\"\\t\"+\"Ok\"\n",
    "                    lines_list.append(info)\n",
    "           \n",
    "    else:\n",
    "        if analysis[\"case\"][0] in cases.keys():\n",
    "            form=analysis[\"number\"][0]+ \" \" + cases[analysis[\"case\"][0]]\n",
    "            new_word=synthesize(i2[0].lemma, form=form)\n",
    "        else:\n",
    "            form=analysis[\"number\"][0]+ \" \" + analysis[\"case\"][0]\n",
    "            new_word=synthesize(i2[0].lemma, form=form)\n",
    "        if len(new_word)>0 and i.lower() in new_word:\n",
    "            info=i+\"\\t\"+i+\"\\t\"+\"Ok\"\n",
    "            lines_list.append(info)\n",
    "        else:\n",
    "            if mult_anal==False:\n",
    "                add_manual_correction_if_available(i)\n",
    "\n",
    "def add_manual_correction_if_available(i):\n",
    "    if i in new_analyses.keys():\n",
    "        info=new_analyses[i]\n",
    "        lines_list.append(info)\n",
    "    else:\n",
    "        info=i+\"\\t\"+\"-\"+\"\\t\"+\"Spell_Unknown\"\n",
    "        lines_list.append(info)\n",
    "\n",
    "\n",
    "def synt(i,analysis,tag,mult_anal):\n",
    "    if analysis[\"case\"][0] in cases.keys():\n",
    "        form=analysis[\"number\"][0]+ \" \" + cases[analysis[\"case\"][0]]\n",
    "        new_word=synthesize(analysis[\"lemma\"], form=form)\n",
    "        if len(new_word)>0 and i.lower() in new_word:\n",
    "            info=i+\"\\t\"+i+\"\\t\"+tag\n",
    "            lines_list.append(info)\n",
    "        else:\n",
    "            if mult_anal==False:\n",
    "                add_manual_correction_if_available(i)\n",
    "    else:\n",
    "        form=analysis[\"number\"][0] + \" \" + analysis[\"case\"][0]\n",
    "        new_word=synthesize(analysis[\"lemma\"], form=form)\n",
    "        if len(new_word)>0 and i.lower() in new_word:\n",
    "            info=i+\"\\t\"+i+\"\\t\"+tag\n",
    "            lines_list.append(info)\n",
    "        else:\n",
    "            if mult_anal==False:\n",
    "                add_manual_correction_if_available(i)\n",
    "\n",
    "\n",
    "def verb_check(morph_root,analysis,tagged_i,i,mult_anal):\n",
    "    if morph_root.lemma != None:\n",
    "        morph_root.root=morph_root.root.replace(\"_\",\"\")\n",
    "    if morph_root.lemma != None and analysis[\"lemma\"]==morph_root.root:\n",
    "        info=i+\"\\t\"+tagged_i.text+\"\\t\"+\"Ok\"\n",
    "        lines_list.append(info)\n",
    "    else:\n",
    "        if len(re.findall(\"ä|Ä|ö|Ö|ü|Ü|Õ|õ\", i)) < len(re.findall(\"ä|Ä|ö|Ö|ü|Ü|Õ|õ\", analysis[\"lemma\"])):\n",
    "            if analysis[\"ending\"]==\"0\" and \"tense\" in analysis.keys() and \"polarity\" in analysis.keys() and \"pres\" in analysis[\"tense\"] and \"neg\" in analysis[\"polarity\"]:\n",
    "                new_word=synthesize(analysis[\"lemma\"]+\"ma\", form=\"o\")\n",
    "            else:\n",
    "                new_word=synthesize(analysis[\"lemma\"]+\"ma\", form=analysis[\"ending\"])\n",
    "            if len(new_word)>0:\n",
    "                if i.lower() in new_word:\n",
    "                    info=i+\"\\t\"+i+\"\\t\"+\"Spell_Missing_Diacritics\"\n",
    "                    lines_list.append(info)\n",
    "                else:\n",
    "                    if i[0].isupper():\n",
    "                        info=i+\"\\t\"+new_word[-1].capitalize()+\"\\t\"+\"Spell_Missing_Diacritics\"\n",
    "                        lines_list.append(info)\n",
    "                    else:\n",
    "                        info=i+\"\\t\"+new_word[-1]+\"\\t\"+\"Spell_Missing_Diacritics\"\n",
    "                        lines_list.append(info)\n",
    "            else:\n",
    "                if mult_anal==False:\n",
    "                    add_manual_correction_if_available(i)        \n",
    "     \n",
    "        else: \n",
    "            if analysis[\"ending\"]==\"0\" and \"tense\" in analysis.keys() and \"polarity\" in analysis.keys() and \"pres\" in analysis[\"tense\"] and \"neg\" in analysis[\"polarity\"]:\n",
    "                new_word=synthesize(analysis[\"lemma\"]+\"ma\", form=\"o\")\n",
    "            else:\n",
    "                new_word=synthesize(analysis[\"lemma\"]+\"ma\", form=analysis[\"ending\"])\n",
    "            if len(new_word)>0:\n",
    "                ld=distance(i.lower(), new_word[-1].lower())\n",
    "                if i.lower() in new_word:\n",
    "                    info=i+\"\\t\"+i+\"\\t\"+\"Ok\"\n",
    "                    lines_list.append(info)\n",
    "                elif ld > 1:\n",
    "                    if i in new_analyses.keys():\n",
    "                        info=new_analyses[i]\n",
    "                        lines_list.append(info)\n",
    "                    else:\n",
    "                        if i[0].isupper():\n",
    "                            info=i+\"\\t\"+new_word[-1].capitalize()+\"\\t\"+\"Spell_ED>1\"\n",
    "                            lines_list.append(info)\n",
    "                        else:\n",
    "                            info=i+\"\\t\"+new_word[-1]+\"\\t\"+\"Spell_ED>1\"\n",
    "                            lines_list.append(info)\n",
    "                elif ld == 1:\n",
    "                    if i[0].isupper():\n",
    "                        info=i+\"\\t\"+new_word[-1].capitalize()+\"\\t\"+\"Spell_ED_1\"\n",
    "                        lines_list.append(info)\n",
    "                    else:\n",
    "                        info=i+\"\\t\"+new_word[-1]+\"\\t\"+\"Spell_ED_1\"\n",
    "                        lines_list.append(info)\n",
    "                elif ld == 0:\n",
    "                    if i[0].isupper():\n",
    "                        info=i+\"\\t\"+new_word[-1].capitalize()+\"\\t\"+\"Ok\"\n",
    "                        lines_list.append(info)\n",
    "                    else:\n",
    "                        info=i+\"\\t\"+new_word[-1]+\"\\t\"+\"Ok\"\n",
    "                        lines_list.append(info)\n",
    "                else:\n",
    "                    if mult_anal==False:\n",
    "                        add_manual_correction_if_available(i)        \n",
    "            else:\n",
    "                if mult_anal==False:\n",
    "                    add_manual_correction_if_available(i)\n",
    "                \n",
    "            \n",
    "def others_check(morph,analysis,tagged_i,i,mult_anal):\n",
    "    if (\"unknown_attribute\" in analysis.keys() and \"Emo\" in analysis[\"unknown_attribute\"]) or (analysis[\"partofspeech\"] ==\"E\"):\n",
    "        info=i+\"\\t\"+analysis[\"lemma\"]+\"\\t\"+\"Emo\"\n",
    "        lines_list.append(info)\n",
    "    elif analysis[\"partofspeech\"]==\"Z\":\n",
    "        info=i+\"\\t\"+analysis[\"lemma\"]+\"\\t\"+\"Punct\"\n",
    "        lines_list.append(info)                  \n",
    "    else:\n",
    "        if \"subtype\" in analysis.keys() and \"prop\" in analysis[\"subtype\"] and not re.search(\"\\d\", analysis[\"lemma\"]):\n",
    "            if analysis[\"case\"][0] in cases.keys():\n",
    "                form=analysis[\"number\"][0]+ \" \" + cases[analysis[\"case\"][0]]\n",
    "                new_word=synthesize(analysis[\"lemma\"], form=form)\n",
    "            else:\n",
    "                form=analysis[\"number\"][0] + \" \" + analysis[\"case\"][0] \n",
    "                new_word=synthesize(analysis[\"lemma\"], form=form)\n",
    "            if len(new_word)>0:\n",
    "                if i.lower() in new_word:\n",
    "                    info=i+\"\\t\"+i+\"\\t\"+\"Name\"\n",
    "                    lines_list.append(info) \n",
    "                elif analysis[\"case\"][0] ==\"gen\" or analysis[\"case\"][0] ==\"part\":\n",
    "                    if new_word[-1][-1] in [\"a\",\"e\",\"i\",\"o\",\"u\",\"õ\",\"ä\",\"ö\"] and new_word[-1][-1]!=i[-1]:\n",
    "                        if analysis[\"case\"][0]==\"gen\":\n",
    "                            info=i+\"\\t\"+new_word[-1]+\"\\t\"+\"Spell_Unknown_Gen\"\n",
    "                            lines_list.append(info) \n",
    "                        else:\n",
    "                            info=i+\"\\t\"+new_word[-1]+\"\\t\"+\"Spell_Unknown_Part\"\n",
    "                            lines_list.append(info) \n",
    "                    else:\n",
    "                        info=i+\"\\t\"+new_word[-1].capitalize()+\"\\t\"+\"Name\"\n",
    "                        lines_list.append(info) \n",
    "                else:\n",
    "                    info=i+\"\\t\"+new_word[-1].capitalize()+\"\\t\"+\"Name\"\n",
    "                    lines_list.append(info) \n",
    "            else:\n",
    "                if mult_anal==False:\n",
    "                    add_manual_correction_if_available(i)\n",
    "             \n",
    "        elif (\"subtype\" in analysis.keys() and \"prop\" in analysis[\"subtype\"]) or ((analysis[\"partofspeech\"]==\"Y\" or analysis[\"partofspeech\"]==\"S\") and re.search(\"\\d\", analysis[\"lemma\"])):\n",
    "            if ((\"case\" not in analysis.keys()) or (\"case\" in analysis.keys() and \"nom\" in analysis[\"case\"]) and \"sg\" in analysis[\"number\"]):\n",
    "                if i in new_analyses.keys(): \n",
    "                    info=new_analyses[i]\n",
    "                    lines_list.append(info)\n",
    "                else:\n",
    "                    info=i+\"\\t\"+analysis[\"lemma\"]+\"\\t\"+\"Abbrev_Num\"\n",
    "                    lines_list.append(info)         \n",
    "            else: \n",
    "                if i.lower()==analysis[\"lemma\"].lower():\n",
    "                    info=i+\"\\t\"+analysis[\"lemma\"]+\"\\t\"+\"Abbrev_Num\"\n",
    "                    lines_list.append(info) \n",
    "                else:\n",
    "                    if mult_anal==False:\n",
    "                        add_manual_correction_if_available(i)               \n",
    "        \n",
    "        elif analysis[\"partofspeech\"]==\"Y\" and not re.search(\"\\d\", analysis[\"lemma\"]):\n",
    "            if (\"case\" in analysis.keys() and \"nom\" in analysis[\"case\"] and \"sg\" in analysis[\"number\"]) or (\"case\" not in analysis.keys()):\n",
    "                info=i+\"\\t\"+analysis[\"lemma\"]+\"\\t\"+\"Abbrev\"\n",
    "                lines_list.append(info)\n",
    "            else: \n",
    "                if \"case\" in analysis.keys():\n",
    "                    if analysis[\"case\"][0] in cases.keys():\n",
    "                        new_word=synthesize(analysis[\"lemma\"], form=analysis[\"number\"][0]+\" \"+cases[analysis[\"case\"][0]])\n",
    "                    else:\n",
    "                        new_word=synthesize(analysis[\"lemma\"], form=analysis[\"number\"][0]+\" \"+analysis[\"case\"][0])\n",
    "                    if len(new_word)>0:\n",
    "                        info=i+\"\\t\"+new_word[-1]+\"\\t\"+\"Abbrev\"\n",
    "                        lines_list.append(info)\n",
    "                    else:\n",
    "                        if mult_anal==False:\n",
    "                            add_manual_correction_if_available(i)                \n",
    "                else:\n",
    "                    if mult_anal==False:\n",
    "                        add_manual_correction_if_available(i)        \n",
    "                    \n",
    "        else:\n",
    "            if morph.lemma!=None and analysis[\"lemma\"].lower() == morph.lemma.lower():\n",
    "                info=i+\"\\t\"+tagged_i.text+\"\\t\"+\"Ok\"\n",
    "                lines_list.append(info)\n",
    "        \n",
    "            elif (\"-\" in i or \":\" in i or \"/\" in i):\n",
    "                if i == analysis[\"lemma\"]:\n",
    "                    info=i+\"\\t\"+analysis[\"lemma\"]+\"\\t\"+\"Word_w_Punct\"\n",
    "                    lines_list.append(info)\n",
    "                else:\n",
    "                    if \"case\" in analysis.keys():\n",
    "                        if mult_anal==False:\n",
    "                            synt(i,analysis,\"Word_w_Punct\",mult_anal==False)\n",
    "                        elif mult_anal==True:\n",
    "                            synt(i,analysis,\"Word_w_Punct\",mult_anal==True)\n",
    "                    else:\n",
    "                        if \"partofspeech\" in analysis.keys() and (analysis[\"partofspeech\"]==\"J\" or analysis[\"partofspeech\"]==\"K\" or analysis[\"partofspeech\"]==\"D\" or analysis[\"partofspeech\"]==\"B\"):\n",
    "                            if analysis[\"lemma\"].lower()==i.lower():\n",
    "                                info=i+\"\\t\"+analysis[\"lemma\"]+\"\\t\"+\"Ok\"\n",
    "                                lines_list.append(info)\n",
    "                            else:\n",
    "                                ld=distance(i.lower(),analysis[\"lemma\"].lower())\n",
    "                                if ld>1:\n",
    "                                    if i[0].isupper():\n",
    "                                        info=i+\"\\t\"+analysis[\"lemma\"].capitalize()+\"\\t\"+\"Spell_ED>1\"\n",
    "                                        lines_list.append(info)\n",
    "                                    else:\n",
    "                                        info=i+\"\\t\"+analysis[\"lemma\"]+\"\\t\"+\"Spell_ED>1\"\n",
    "                                        lines_list.append(info)\n",
    "                                elif ld == 1:\n",
    "                                    if i[0].isupper():\n",
    "                                        info=i+\"\\t\"+analysis[\"lemma\"].capitalize()+\"\\t\"+\"Spell_ED_1\"\n",
    "                                        lines_list.append(info)\n",
    "                                    else:\n",
    "                                        info=i+\"\\t\"+analysis[\"lemma\"]+\"\\t\"+\"Spell_ED_1\"\n",
    "                                        lines_list.append(info)\n",
    "                                else:\n",
    "                                    if mult_anal==False:\n",
    "                                        add_manual_correction_if_available(i)\n",
    "                        else:\n",
    "                            if mult_anal==False:\n",
    "                                add_manual_correction_if_available(i)\n",
    "                            \n",
    "            else:\n",
    "                if \"partofspeech\" in analysis.keys() and (analysis[\"partofspeech\"]==\"J\" or analysis[\"partofspeech\"]==\"K\" or analysis[\"partofspeech\"]==\"D\" or analysis[\"partofspeech\"]==\"B\"):\n",
    "                    if analysis[\"lemma\"].lower()==i.lower():\n",
    "                        info=i+\"\\t\"+analysis[\"lemma\"]+\"\\t\"+\"Ok\"\n",
    "                        lines_list.append(info)\n",
    "                    else:\n",
    "                        if len(re.findall(\"ä|Ä|ö|Ö|ü|Ü|Õ|õ\", i)) < len(re.findall(\"ä|Ä|ö|Ö|ü|Ü|Õ|õ\", analysis[\"lemma\"])):\n",
    "                            if i[0].isupper():\n",
    "                                info=i+\"\\t\"+analysis[\"lemma\"].capitalize()+\"\\t\"+\"Spell_Missing_Diacritics\"\n",
    "                                lines_list.append(info) \n",
    "                            else:\n",
    "                                info=i+\"\\t\"+analysis[\"lemma\"]+\"\\t\"+\"Spell_Missing_Diacritics\"\n",
    "                                lines_list.append(info) \n",
    "                        else:\n",
    "                            ld=distance(i.lower(),analysis[\"lemma\"].lower())\n",
    "                            if ld>1:\n",
    "                                if i[0].isupper():\n",
    "                                    info=i+\"\\t\"+analysis[\"lemma\"].capitalize()+\"\\t\"+\"Spell_ED>1\"\n",
    "                                    lines_list.append(info)\n",
    "                                else:\n",
    "                                    info=i+\"\\t\"+analysis[\"lemma\"]+\"\\t\"+\"Spell_ED>1\"\n",
    "                                    lines_list.append(info)\n",
    "                            elif ld == 1:\n",
    "                                if i[0].isupper():\n",
    "                                    info=i+\"\\t\"+analysis[\"lemma\"].capitalize()+\"\\t\"+\"Spell_ED_1\"\n",
    "                                    lines_list.append(info)\n",
    "                                else:\n",
    "                                    info=i+\"\\t\"+analysis[\"lemma\"]+\"\\t\"+\"Spell_ED_1\"\n",
    "                                    lines_list.append(info)\n",
    "                            else:\n",
    "                                if mult_anal==False:\n",
    "                                    add_manual_correction_if_available(i)\n",
    "                    \n",
    "                elif \"case\" in analysis.keys():\n",
    "                    if analysis[\"case\"][0] in cases.keys():\n",
    "                        form=analysis[\"number\"][0]+ \" \" + cases[analysis[\"case\"][0]]\n",
    "                    else:\n",
    "                        form=analysis[\"number\"][0] + \" \" + analysis[\"case\"][0]\n",
    "                    if mult_anal==False:\n",
    "                        use_edit_distance(analysis,form, i, i2, mult_anal==False)\n",
    "                    elif mult_anal==True:\n",
    "                        use_edit_distance(analysis,form, i, i2, mult_anal==True)\n",
    "\n",
    "                else:\n",
    "                    if i.lower() == analysis[\"lemma\"].lower():\n",
    "                        info=i+\"\\t\"+analysis[\"lemma\"]+\"\\t\"+\"Ok\"\n",
    "                        lines_list.append(info)\n",
    "                    else:\n",
    "                        if mult_anal==False:\n",
    "                            add_manual_correction_if_available(i)\n",
    "                        \n",
    "                            \n",
    "\n",
    "for file in os.listdir(path):\n",
    "    filename = os.path.join(path, file)\n",
    "    if \".cg\" in filename:\n",
    "        print(\"******************FAILINIMI:\",filename)\n",
    "        with open (filename, 'r', encoding=\"utf8\") as f:\n",
    "            lines_list=[]\n",
    "            new_filename=filename.split(\"\\\\\")[-1]\n",
    "            new_filename=new_filename.split(\".\")\n",
    "            new_filename=new_filename[-2]\n",
    "            new_row=[]\n",
    "            text=[]\n",
    "            analyses=[]\n",
    "            row=f.read()\n",
    "            row=row.split(\"\\n\")\n",
    "            for i in row:\n",
    "                if not i.startswith('\"<s>\"') and not i.startswith('\"</s>\"') and i != '' and not i.startswith('\"<kiil>\"') and not i.startswith('\"</kiil>\"') and not i.startswith('\"<kindel_piir/>\"') and not i.startswith('<kindel_piir/>\"') and not \"####\" in i :\n",
    "                    new_row.append(i)\n",
    "            for i in range(1, len(new_row), 2):\n",
    "                analyses.append(new_row[i])\n",
    "            for i in range(0, len(new_row), 2):\n",
    "                i=new_row[i].replace(\"<\",\"\")\n",
    "                i=i.replace(\">\",\"\")\n",
    "                i=i[1:].strip()\n",
    "                text.append(i[:-1])\n",
    "            \n",
    "            for i,analysis in zip(text,analyses):\n",
    "                analysis = parser.parse(analysis)\n",
    "                analysis[\"lemma\"]=analysis[\"lemma\"].replace(\"_\",\"\")\n",
    "                analysis[\"lemma\"]=analysis[\"lemma\"].replace(\"=\",\"\")\n",
    "                tagged_i=Text(i)\n",
    "                tagged_i.tag_layer(['words','sentences'])\n",
    "                morph_tagger.tag(tagged_i)\n",
    "                morph_counter=0\n",
    "                for i2 in tagged_i.morph_analysis:\n",
    "                    morph_counter+=1\n",
    "                    if morph_counter==1:\n",
    "                        if i2[0].lemma != None: # if auto.morph analysis != None\n",
    "                            i2[0].lemma=i2[0].lemma.replace(\"_\",\"\")\n",
    "                            i2[0].lemma=i2[0].lemma.replace(\"=\",\"\")\n",
    "                            if len(i2)==1: # one auto.morph analysis\n",
    "                                if analysis[\"partofspeech\"]==\"V\":\n",
    "                                    verb_check(i2[0],analysis,tagged_i,i,mult_anal=False)\n",
    "                                else:\n",
    "                                    others_check(i2[0],analysis,tagged_i,i,mult_anal=False)\n",
    "                            else: # more than one auto.morph analysis\n",
    "                                if analysis[\"partofspeech\"]==\"V\":\n",
    "                                    length=len(lines_list)\n",
    "                                    for listike in i2:\n",
    "                                        if length==len(lines_list):\n",
    "                                            verb_check(listike,analysis,tagged_i,i,mult_anal=True)\n",
    "                                    if length==len(lines_list):\n",
    "                                        add_manual_correction_if_available(i)\n",
    "                                else:\n",
    "                                    length=len(lines_list)\n",
    "                                    for listike in i2:\n",
    "                                        if length==len(lines_list):\n",
    "                                            others_check(listike,analysis,tagged_i,i,mult_anal=True)\n",
    "                                    if length==len(lines_list):\n",
    "                                        add_manual_correction_if_available(i)\n",
    "                        else: # if auto.morph analysis == None\n",
    "                            if analysis[\"partofspeech\"]==\"V\":\n",
    "                                verb_check(i2[0],analysis,tagged_i,i,mult_anal=False)\n",
    "                            else:\n",
    "                                others_check(i2[0],analysis,tagged_i,i,mult_anal=False)\n",
    "                            \n",
    "                            \n",
    "            with open(new_path + new_filename+\".tsv\",\"w\",encoding=\"utf8\") as f:\n",
    "                for line in lines_list:\n",
    "                    f.write(line+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
